{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289},{"sourceId":1373314,"sourceType":"datasetVersion","datasetId":798371},{"sourceId":3508078,"sourceType":"datasetVersion","datasetId":2111202},{"sourceId":3514629,"sourceType":"datasetVersion","datasetId":2113070}],"dockerImageVersionId":30178,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0. Install important libraries","metadata":{}},{"cell_type":"code","source":"%%capture\n# Install libraries\n!pip install tensorflow keras pillow numpy tqdm underthesea gensim fasttext nltk","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-23T16:24:09.56203Z","iopub.execute_input":"2022-04-23T16:24:09.562359Z","iopub.status.idle":"2022-04-23T16:24:40.106602Z","shell.execute_reply.started":"2022-04-23T16:24:09.562256Z","shell.execute_reply":"2022-04-23T16:24:40.105522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Import important libraries","metadata":{}},{"cell_type":"code","source":"%%capture\nimport os\nimport sys\nimport string\nimport fasttext\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom pickle import dump, load\nfrom underthesea import word_tokenize\nfrom gensim.utils import simple_preprocess\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.preprocessing.sequence import pad_sequences\n\n# from keras.applications.xception import Xception, preprocess_input\nfrom keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom keras.layers import Input, Dense, LSTM, Embedding, Dropout, Bidirectional, add\nfrom keras.initializers import Constant\nfrom keras.models import Model, load_model\nfrom keras.utils.vis_utils import plot_model\n\nfrom tensorflow.keras.layers import Bidirectional\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.utils import to_categorical\n\nfrom tqdm.notebook import tqdm\nfrom nltk.translate.bleu_score import corpus_bleu","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:24:40.108536Z","iopub.execute_input":"2022-04-23T16:24:40.108764Z","iopub.status.idle":"2022-04-23T16:24:46.256045Z","shell.execute_reply.started":"2022-04-23T16:24:40.108731Z","shell.execute_reply":"2022-04-23T16:24:46.25532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Getting and performing data cleaning","metadata":{}},{"cell_type":"code","source":"def load_doc(filename):\n    file = open(filename, 'r', encoding='utf-8')\n    text = file.read()\n    file.close()\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:24:46.257427Z","iopub.execute_input":"2022-04-23T16:24:46.257716Z","iopub.status.idle":"2022-04-23T16:24:46.262096Z","shell.execute_reply.started":"2022-04-23T16:24:46.257681Z","shell.execute_reply":"2022-04-23T16:24:46.26155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def all_img_captions(filename):\n    file = load_doc(filename)\n    captions = file.split('\\n')\n    descriptions = dict()\n    for caption in captions[:-1]:\n        img, cap = caption.split('\\t')\n        if img not in descriptions:\n            descriptions[img] = [cap]\n        else:\n            descriptions[img].append(cap)\n    return descriptions","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:24:46.263974Z","iopub.execute_input":"2022-04-23T16:24:46.26442Z","iopub.status.idle":"2022-04-23T16:24:46.280622Z","shell.execute_reply.started":"2022-04-23T16:24:46.264383Z","shell.execute_reply":"2022-04-23T16:24:46.279892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cleaning_text(descriptions):\n    table = str.maketrans('', '', string.punctuation)\n    for img, captions in descriptions.items():\n        for idx, cap in enumerate(captions):\n            desc = cap.replace(' - ', ' ').translate(table)\n            desc = ' '.join(simple_preprocess(desc))\n            desc = word_tokenize(desc, format='text')\n            \n            descriptions[img][idx] = desc\n    return descriptions","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:24:46.28179Z","iopub.execute_input":"2022-04-23T16:24:46.282146Z","iopub.status.idle":"2022-04-23T16:24:46.291097Z","shell.execute_reply.started":"2022-04-23T16:24:46.28211Z","shell.execute_reply":"2022-04-23T16:24:46.29032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_vocabulary(descriptions):\n    vocab = set()\n    for key in descriptions.keys():\n        [vocab.update(desc.split()) for desc in descriptions[key]]\n    \n    return vocab","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:24:46.292776Z","iopub.execute_input":"2022-04-23T16:24:46.292992Z","iopub.status.idle":"2022-04-23T16:24:46.303504Z","shell.execute_reply.started":"2022-04-23T16:24:46.292958Z","shell.execute_reply":"2022-04-23T16:24:46.302892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_descriptions(descriptions, filename):\n    lines = list()\n    for key, captions in descriptions.items():\n        for cap in captions:\n            lines.append(key + '\\t' + cap)\n    data = '\\n'.join(lines)\n    file = open(filename, 'w', encoding='utf-8')\n    file.write(data)\n    file.close()","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:24:46.304701Z","iopub.execute_input":"2022-04-23T16:24:46.305034Z","iopub.status.idle":"2022-04-23T16:24:46.31288Z","shell.execute_reply.started":"2022-04-23T16:24:46.304997Z","shell.execute_reply":"2022-04-23T16:24:46.31229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_text = '../input/flickr8k-vi-caps'\ndataset_images = '../input/flickr8k/Images'\nsample_image = '2423292784_166ee54e0b.jpg'\n\ntext_file = dataset_text + '/' + 'captions_vi.txt'","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:24:46.313769Z","iopub.execute_input":"2022-04-23T16:24:46.313944Z","iopub.status.idle":"2022-04-23T16:24:46.32262Z","shell.execute_reply.started":"2022-04-23T16:24:46.313923Z","shell.execute_reply":"2022-04-23T16:24:46.321921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"descriptions = all_img_captions(text_file)\nprint('Length of descriptions:', len(descriptions))\nprint('Description before clean text')\ndescriptions[sample_image]","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:24:46.324117Z","iopub.execute_input":"2022-04-23T16:24:46.32462Z","iopub.status.idle":"2022-04-23T16:24:46.478518Z","shell.execute_reply.started":"2022-04-23T16:24:46.324584Z","shell.execute_reply":"2022-04-23T16:24:46.477787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_descriptions = cleaning_text(descriptions)\nprint('Descriptions after clean text')\nclean_descriptions[sample_image]","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:24:46.481737Z","iopub.execute_input":"2022-04-23T16:24:46.482154Z","iopub.status.idle":"2022-04-23T16:25:14.246322Z","shell.execute_reply.started":"2022-04-23T16:24:46.482112Z","shell.execute_reply":"2022-04-23T16:25:14.245506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = text_vocabulary(clean_descriptions)\nprint('Length of vocabuary:', len(vocab))","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:25:14.247846Z","iopub.execute_input":"2022-04-23T16:25:14.248338Z","iopub.status.idle":"2022-04-23T16:25:14.335622Z","shell.execute_reply.started":"2022-04-23T16:25:14.248298Z","shell.execute_reply":"2022-04-23T16:25:14.334945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_descriptions(clean_descriptions, 'descriptions.txt')","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:25:14.336821Z","iopub.execute_input":"2022-04-23T16:25:14.337051Z","iopub.status.idle":"2022-04-23T16:25:14.381917Z","shell.execute_reply.started":"2022-04-23T16:25:14.337019Z","shell.execute_reply":"2022-04-23T16:25:14.381245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Extract feature vector from all images","metadata":{}},{"cell_type":"code","source":"def extract_feature(directory):\n    model = InceptionV3(weights='imagenet')\n    model = Model(model.input, model.layers[-2].output)\n    \n    features = dict()\n    for img in tqdm(os.listdir(directory), file=sys.stdout):\n        filename = directory + '/' + img\n        image = load_img(filename, target_size=(299, 299))\n        image = img_to_array(image)\n        image = np.expand_dims(image, axis=0)\n        image = preprocess_input(image)\n        \n        feature = model.predict(image, verbose=0)\n        features[img] = feature\n    return features","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:25:14.383085Z","iopub.execute_input":"2022-04-23T16:25:14.383347Z","iopub.status.idle":"2022-04-23T16:25:14.393025Z","shell.execute_reply.started":"2022-04-23T16:25:14.383312Z","shell.execute_reply":"2022-04-23T16:25:14.392243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# features = extract_feature(dataset_images)\n# dump(features, open('features_inception_v3.pkl', 'wb'))","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:25:14.39435Z","iopub.execute_input":"2022-04-23T16:25:14.394861Z","iopub.status.idle":"2022-04-23T16:25:14.405855Z","shell.execute_reply.started":"2022-04-23T16:25:14.394822Z","shell.execute_reply":"2022-04-23T16:25:14.405082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_path = '../input/flickr8k-feature-vector/features_inception_v3.pkl'\nfeatures = load(open(feature_path, 'rb'))","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:25:14.407736Z","iopub.execute_input":"2022-04-23T16:25:14.408335Z","iopub.status.idle":"2022-04-23T16:25:15.569787Z","shell.execute_reply.started":"2022-04-23T16:25:14.408296Z","shell.execute_reply":"2022-04-23T16:25:15.568952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features[sample_image]","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:25:15.570956Z","iopub.execute_input":"2022-04-23T16:25:15.571217Z","iopub.status.idle":"2022-04-23T16:25:15.578101Z","shell.execute_reply.started":"2022-04-23T16:25:15.571184Z","shell.execute_reply":"2022-04-23T16:25:15.577306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Loading dataset for training the model","metadata":{}},{"cell_type":"code","source":"def load_photos(filename):\n    file = load_doc(filename)\n    photos = file.split('\\n')\n    return photos","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:25:15.579789Z","iopub.execute_input":"2022-04-23T16:25:15.580889Z","iopub.status.idle":"2022-04-23T16:25:15.587744Z","shell.execute_reply.started":"2022-04-23T16:25:15.580669Z","shell.execute_reply":"2022-04-23T16:25:15.587094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_descriptions(filename, photos):\n    file = load_doc(filename)\n    descriptions = dict()\n    for line in file.split('\\n'):\n        line = line.split('\\t')\n        \n        if len(line) < 1:\n            continue\n            \n        image, cap = line[0], line[1]\n        if image in photos:\n            if image not in descriptions:\n                descriptions[image] = list()\n            desc = 'startseq ' + cap + ' endseq'\n            descriptions[image].append(desc)\n    return descriptions","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:25:15.589376Z","iopub.execute_input":"2022-04-23T16:25:15.589737Z","iopub.status.idle":"2022-04-23T16:25:15.598446Z","shell.execute_reply.started":"2022-04-23T16:25:15.589692Z","shell.execute_reply":"2022-04-23T16:25:15.597703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_features(photos):\n    all_features = load(open(feature_path, 'rb'))\n    features = {img: all_features[img] for img in photos}\n    return features","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:25:15.599936Z","iopub.execute_input":"2022-04-23T16:25:15.600355Z","iopub.status.idle":"2022-04-23T16:25:15.607708Z","shell.execute_reply.started":"2022-04-23T16:25:15.600321Z","shell.execute_reply":"2022-04-23T16:25:15.606956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_imgs = load_photos(dataset_text + '/' + 'trainImages.txt')\ntrain_descriptions = load_descriptions('descriptions.txt', train_imgs)\ntrain_features = load_features(train_imgs)\n\ntest_imgs = load_photos(dataset_text + '/' + 'testImages.txt')\ntest_descriptions = load_descriptions('descriptions.txt', test_imgs)\ntest_features = load_features(test_imgs)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:25:15.610079Z","iopub.execute_input":"2022-04-23T16:25:15.610876Z","iopub.status.idle":"2022-04-23T16:25:20.101001Z","shell.execute_reply.started":"2022-04-23T16:25:15.610834Z","shell.execute_reply":"2022-04-23T16:25:20.100068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Tokenizing the vocabulary and word embedding","metadata":{"execution":{"iopub.status.busy":"2022-04-22T06:46:01.830847Z","iopub.execute_input":"2022-04-22T06:46:01.831106Z","iopub.status.idle":"2022-04-22T06:46:01.836967Z","shell.execute_reply.started":"2022-04-22T06:46:01.831076Z","shell.execute_reply":"2022-04-22T06:46:01.836194Z"}}},{"cell_type":"code","source":"def dict_to_list(descriptions):\n    all_desc = list()\n    for key in descriptions.keys():\n        [all_desc.append(desc) for desc in descriptions[key]]\n    return all_desc","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:25:20.104622Z","iopub.execute_input":"2022-04-23T16:25:20.105107Z","iopub.status.idle":"2022-04-23T16:25:20.12005Z","shell.execute_reply.started":"2022-04-23T16:25:20.105036Z","shell.execute_reply":"2022-04-23T16:25:20.118213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_tokenizer(descriptions):\n    desc_list = dict_to_list(descriptions)\n    oov_token = 'unk'\n    tokenizer = Tokenizer(oov_token=oov_token)\n    tokenizer.fit_on_texts(desc_list)\n    return tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:25:20.123329Z","iopub.execute_input":"2022-04-23T16:25:20.124091Z","iopub.status.idle":"2022-04-23T16:25:20.135399Z","shell.execute_reply.started":"2022-04-23T16:25:20.124042Z","shell.execute_reply":"2022-04-23T16:25:20.134642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = create_tokenizer(train_descriptions)\ndump(tokenizer, open('tokenizer.pkl', 'wb'))\nvocab_size = len(tokenizer.word_index) + 1\nprint('# vocabulary:', vocab_size)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:25:20.136929Z","iopub.execute_input":"2022-04-23T16:25:20.137246Z","iopub.status.idle":"2022-04-23T16:25:21.186948Z","shell.execute_reply.started":"2022-04-23T16:25:20.137207Z","shell.execute_reply":"2022-04-23T16:25:21.186102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def max_length(descriptions):\n    desc_list = dict_to_list(descriptions)\n    return max(len(desc.split()) for desc in desc_list)\nmax_length = max_length(descriptions)\nprint('Max length of train set:', max_length)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:25:21.188283Z","iopub.execute_input":"2022-04-23T16:25:21.188699Z","iopub.status.idle":"2022-04-23T16:25:21.253644Z","shell.execute_reply.started":"2022-04-23T16:25:21.18866Z","shell.execute_reply":"2022-04-23T16:25:21.252928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_embedding_matrix(vocab):\n    embedding_index = fasttext.load_model('../input/fasttext-vietnamese-word-vectors-full/cc.vi.300.bin')\n    \n    embedding_dim = 300\n    hits = 0\n    misses = 0\n    vocab_size = len(vocab) + 1\n    \n    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n    for word, idx in vocab.items():\n        embedding_vector = embedding_index[word]\n        if embedding_vector is not None:\n            embedding_matrix[idx] = embedding_vector\n            hits += 1\n        else:\n            misses += 1\n    print(\"Converted %d words (%d misses)\" % (hits, misses))\n    \n    return embedding_matrix","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:25:21.254892Z","iopub.execute_input":"2022-04-23T16:25:21.255129Z","iopub.status.idle":"2022-04-23T16:25:21.262055Z","shell.execute_reply.started":"2022-04-23T16:25:21.255099Z","shell.execute_reply":"2022-04-23T16:25:21.261215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture --no-stdout\nvocab = tokenizer.word_index\nembedding_matrix = create_embedding_matrix(vocab)\nprint(embedding_matrix.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:25:21.263749Z","iopub.execute_input":"2022-04-23T16:25:21.264271Z","iopub.status.idle":"2022-04-23T16:26:18.284251Z","shell.execute_reply.started":"2022-04-23T16:25:21.264234Z","shell.execute_reply":"2022-04-23T16:26:18.283541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Create data generator","metadata":{}},{"cell_type":"code","source":"def data_generator(descriptions, features, tokenizer, max_length, vocab_size, batch_size):\n    X1, X2, y = list(), list(), list()\n    n = 0\n    \n    while True:\n        for key, captions in descriptions.items():\n            n += 1\n            feature = features[key][0]\n            for caption in captions:\n                seq = tokenizer.texts_to_sequences([caption])[0]\n                for i in range(1, len(seq)):\n                    in_seq, out_seq = seq[:i], seq[i]\n                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n\n                    X1.append(feature)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n            if n == batch_size:\n                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n                yield [X1, X2], y\n                X1, X2, y = list(), list(), list()\n                n = 0","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:26:18.285921Z","iopub.execute_input":"2022-04-23T16:26:18.286173Z","iopub.status.idle":"2022-04-23T16:26:18.296757Z","shell.execute_reply.started":"2022-04-23T16:26:18.286138Z","shell.execute_reply":"2022-04-23T16:26:18.294883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[a, b], c = next(data_generator(train_descriptions, features, tokenizer, max_length, vocab_size, 32))\na.shape, b.shape, c.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:26:18.301022Z","iopub.execute_input":"2022-04-23T16:26:18.301238Z","iopub.status.idle":"2022-04-23T16:26:18.409906Z","shell.execute_reply.started":"2022-04-23T16:26:18.301215Z","shell.execute_reply":"2022-04-23T16:26:18.409215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Defining the CNN-RNN model","metadata":{}},{"cell_type":"code","source":"def define_model(vocab_size, max_length):\n    embedding_dim = 300\n    inputs1 = Input(shape=(2048,))\n    fe1 = Dropout(0.5)(inputs1)\n    fe2 = Dense(256, activation='relu')(fe1)\n\n    inputs2 = Input(shape=(max_length,))\n    se1 = Embedding(\n        vocab_size,\n        embedding_dim,\n        embeddings_initializer=Constant(embedding_matrix),\n        mask_zero=True,\n        trainable=False,\n    )(inputs2)\n    se2 = Dropout(0.5)(se1)\n    se3 = Bidirectional(LSTM(128))(se2)\n\n    decoder1 = add([fe2, se3])\n    decoder2 = Dense(256, activation='relu')(decoder1)\n    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n\n    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n    \n    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0001))\n\n    print(model.summary())\n    plot_model(model, to_file='model.png', show_shapes=True)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:26:18.411245Z","iopub.execute_input":"2022-04-23T16:26:18.411511Z","iopub.status.idle":"2022-04-23T16:26:18.419768Z","shell.execute_reply.started":"2022-04-23T16:26:18.411477Z","shell.execute_reply":"2022-04-23T16:26:18.418956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Training the model","metadata":{}},{"cell_type":"code","source":"print('Dataset: ', len(train_imgs))\nprint('Descriptions: train=', len(train_descriptions))\nprint('Photos: train=', len(train_features))\nprint('Vocabulary Size:', vocab_size)\nprint('Description Length: ', max_length)\n\nmodel = define_model(vocab_size, max_length)\nepochs = 50\nbatch_size = 32\nsteps = len(train_imgs) // batch_size","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:26:18.421208Z","iopub.execute_input":"2022-04-23T16:26:18.421624Z","iopub.status.idle":"2022-04-23T16:26:23.355137Z","shell.execute_reply.started":"2022-04-23T16:26:18.421559Z","shell.execute_reply":"2022-04-23T16:26:23.354301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(epochs):\n    generator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size, batch_size)\n    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n\nmodel.save(\"best_model.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:26:23.357246Z","iopub.execute_input":"2022-04-23T16:26:23.357546Z","iopub.status.idle":"2022-04-23T17:21:05.237253Z","shell.execute_reply.started":"2022-04-23T16:26:23.357507Z","shell.execute_reply":"2022-04-23T17:21:05.236545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. Generate captions for the image","metadata":{}},{"cell_type":"code","source":"def idx_to_word(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None","metadata":{"execution":{"iopub.status.busy":"2022-04-23T17:21:05.238728Z","iopub.execute_input":"2022-04-23T17:21:05.238994Z","iopub.status.idle":"2022-04-23T17:21:05.244939Z","shell.execute_reply.started":"2022-04-23T17:21:05.238957Z","shell.execute_reply":"2022-04-23T17:21:05.242891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_caption(model, image, tokenizer, max_length):\n    in_text = 'startseq'\n    for i in range(max_length):\n        seq = tokenizer.texts_to_sequences([in_text])[0]\n        seq = pad_sequences([seq], maxlen=max_length)\n        yhat = model.predict([image, seq], verbose=0)\n        pred_ids = np.argmax(yhat)\n        word = idx_to_word(pred_ids, tokenizer)\n        if word is None:\n            break\n        in_text += ' ' + word\n        if word == 'endseq':\n            break\n    return in_text","metadata":{"execution":{"iopub.status.busy":"2022-04-23T17:21:05.246344Z","iopub.execute_input":"2022-04-23T17:21:05.246597Z","iopub.status.idle":"2022-04-23T17:21:05.257538Z","shell.execute_reply.started":"2022-04-23T17:21:05.246561Z","shell.execute_reply":"2022-04-23T17:21:05.256872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = load_model('./best_model.h5')\n# actual, predicted = list(), list()\n\n# for image in tqdm(test_imgs):\n#     captions = test_descriptions[image]\n#     actual_captions = [caption.split() for caption in captions]\n\n#     y_pred = predict_caption(model, test_features[image], tokenizer, max_length)\n#     y_pred = y_pred.split()\n\n#     actual.append(actual_captions)\n#     predicted.append(y_pred)\n\n# # BLEU score\n# print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n# print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))","metadata":{"execution":{"iopub.status.busy":"2022-04-23T17:21:05.258826Z","iopub.execute_input":"2022-04-23T17:21:05.259114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. Visualize the results","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\n\ndef generate_caption(image_name):\n    image_path = os.path.join(dataset_images, image_name)\n    image = Image.open(image_path)\n    descriptions = load_descriptions('./descriptions.txt', train_imgs + test_imgs)\n    captions = descriptions[image_name]\n    print('-' * 20 + 'Actual' + '-' * 20)\n    for caption in captions:\n        print(caption)\n    y_pred = predict_caption(model, features[image_name], tokenizer, max_length)\n    print('-' * 20 + 'Predicted' + '-' * 20)\n    print(y_pred)\n    plt.imshow(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from random import sample\n\nimage_name = sample(train_imgs, 1)[0]\ngenerate_caption(image_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}